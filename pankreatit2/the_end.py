# -*- coding: utf-8 -*-
"""the_end.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GjfkQpVR2VyqpDDN-vLEzdRE5LKiS8VT
"""

import joblib
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, plot_roc_curve
from sklearn.model_selection import train_test_split, cross_validate
from scipy import stats
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram
from scipy.cluster.hierarchy import linkage
from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

pd.set_option('display.max_columns', 50)
pd.set_option('display.width', 500)
pd.set_option('display.max_rows', 300)
import warnings
warnings.filterwarnings("ignore")

def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):
    quartile1 = dataframe[col_name].quantile(q1)
    quartile3 = dataframe[col_name].quantile(q3)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

def check_outlier(dataframe, col_name, q1=0.25, q3=0.75):
    low_limit, up_limit = outlier_thresholds(dataframe, col_name, q1, q3)
    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):
        return True
    else:
        return False

def replace_with_thresholds(dataframe, variable):
    low_limit, up_limit = outlier_thresholds(dataframe, variable)
    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit
    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit

df=pd.read_excel("pankreatit2/NEWPAN.xlsx")

df.drop([254,255,256,257,258,259,260],axis=0, inplace=True)

df["dep_var"] = df["DEATH"] + df["ICU"] +df["RADIO"]
df["dep_var"].value_counts()
df.dep_var[df["dep_var"]>=1] = 1

for i in df[['LDH','CRP','PCT','PDW','PLCR','NBRC_per','NBRC','UREA']]:
             df[i] = df[i].fillna(df[i].median())


df["UREA"]=df["UREA"].astype(float)
df.drop(["NBRC","NBRC_per"],axis=1, inplace=True)
df.drop(["AML/NEU","LIP/NEU","PLCR","RDW_SD","RDW_CV","PCT","PDW","DEATH","ICU"],axis=1, inplace=True)

num_cols = [col for col in df.columns if df[col].dtypes != "O"]

for col in num_cols:
         outlier_thresholds(df, col)

for col in df[["NEU","WBC","LYM","SII","UREA","CREA","AST","ALT","LDH","HGB","PLT","GLU","AMYLASE","LIPASE","PLR","NLR"]]:
         replace_with_thresholds(df,col)

df["NEW_AMY_LIP"]=df["AMYLASE"]+df["LIPASE"]

df["NEW_WBC_EQL"]=df["WBC"]-df["NEU"]-df["LYM"]

df["NEW_AMY_UREA"]=df["UREA"]/df["AMYLASE"]

df["RADIO_SCORE"]=df["RADIO"]+1

df["RADIO_SCORE"]=df["RADIO_SCORE"]*df["AGE"]

df.drop(["AMYLASE"],axis=1,inplace=True)
df.drop(["GALLSTONE","RADIO"],axis=1, inplace=True)

new_list = [col for col in df.columns if col not in "dep_var"]

X = df[new_list]
y = df["dep_var"]

log_model = LogisticRegression().fit(X, y)

log_model.intercept_
log_model.coef_

y_pred = log_model.predict(X)

y_pred[0:10]

y[0:10]

def plot_confusion_matrix(y, y_pred):
    acc = round(accuracy_score(y, y_pred), 2)
    cm = confusion_matrix(y, y_pred)
    sns.heatmap(cm, annot=True, fmt=".0f")
    plt.xlabel('y_pred')
    plt.ylabel('y')
    plt.title('Accuracy Score: {0}'.format(acc), size=10)
    plt.show()

plot_confusion_matrix(y, y_pred)

print(classification_report(y, y_pred))

y_prob = log_model.predict_proba(X)[:, 1]
roc_auc_score(y, y_prob)

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.20,random_state=45)

log_model = LogisticRegression().fit(X_train, y_train)

y_pred = log_model.predict(X_test)
y_prob = log_model.predict_proba(X_test)[:, 1]

print(classification_report(y_test, y_pred))

plot_roc_curve(log_model, X_test, y_test)
plt.title('ROC Curve')
plt.plot([0, 1], [0, 1], 'r--')
plt.show()

roc_auc_score(y_test, y_prob)

X = df[new_list]
y = df["dep_var"]

log_model = LogisticRegression().fit(X, y)

cv_results = cross_validate(log_model,
                            X, y,
                            cv=10,
                            scoring=["accuracy", "precision", "recall", "f1", "roc_auc"])

cv_results['test_recall'].mean()

a = X.sample(1,random_state=42)
a.columns
a.shape

values = [1.0,24.0,9.8,7.42,1.31, 12.8, 279.0, 2070.18,1580.0,\
64.0,25.3, 0.62,80.0,282.0, 177.0,264.0,26.23,212.0,
5.0,855.0,1.07,0.042809,48.0]
a.values
inputs = ['SEX', 'AGE', 'WBC', 'NEU', 'LYM', 'HGB', 'PLT', 'NEU*PLT', 'SII',
 'GLU', 'UREA', 'CREA', 'AST', 'ALT', 'LDH', 'LIPASE', 'CRP', 'PLR',
 'NLR', 'NEW_AMY_LIP', 'NEW_WBC_EQL', 'NEW_AMY_UREA', 'RADIO_SCORE']
"""--------------------------------buraya kadar bundan sonrasÄ± pickle---------------------------------------"""


import numpy as np
import pandas as pd
import pickle


import os
os.getcwd()
pickle.dump(log_model,open("model.pk1","wb"))
log_model_new = pickle.load(open("/home/rahman/PycharmProjects/pythonProject/pankreatit2/model.pk1","rb"))
log_model_new.predict(a)

"""--------------------------------RANDOMOVERSAMPLER---------------------------------------"""


from imblearn.over_sampling import RandomOverSampler

oversample = RandomOverSampler(sampling_strategy='minority')
X_randomover, y_randomover = oversample.fit_resample(X_train, y_train)

log_model.fit(X_randomover, y_randomover)
y_pred = log_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: %.2f%%" % (accuracy))

print(classification_report(y_test, y_pred))













X_scaled = StandardScaler().fit_transform(X)
X = pd.DataFrame(X_scaled,columns=X.columns)

rf_model = RandomForestClassifier()
rf_model.get_params()
cv_results = cross_validate(rf_model, X, y, cv=10, scoring=["accuracy", "f1", "roc_auc"])
cv_results['test_accuracy'].mean()
cv_results['test_f1'].mean()
cv_results['test_roc_auc'].mean()

rf_params = {"max_depth": [4, 10, None],
             "max_features": [8, 10, 12, "auto"],
             "min_samples_split": [2, 5, 8, 15, 20],
             "n_estimators": [100, 200, 500]}

rf_best_grid = GridSearchCV(rf_model, rf_params, cv=5, n_jobs=-1, verbose=True).fit(X, y)
rf_best_grid.best_params_
rf_final = rf_model.set_params(**rf_best_grid.best_params_).fit(X, y)
cv_results = cross_validate(rf_final, X, y, cv=5, scoring=["accuracy", "f1", "roc_auc"])
cv_results['test_accuracy'].mean()

def base_models(X, y, scoring="roc_auc"):
    print("Base Models....")
    classifiers = [('LR', LogisticRegression()),
                   ("SVC", SVC()),
                   ("RF", RandomForestClassifier()),
                   ('Adaboost', AdaBoostClassifier()),
                   ('GBM', GradientBoostingClassifier()),
                   ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),
                   ('LightGBM', LGBMClassifier()),
                   
                   ]

    for name, classifier in classifiers:
        cv_results = cross_validate(classifier, X, y, cv=3, scoring=scoring)
        print(f"{scoring}: {round(cv_results['test_score'].mean(), 4)} ({name}) ")

base_models(X, y, scoring="precision")

rf_params = {"max_depth": [8, 15, None],
             "max_features": [5, 7, "auto"],
             "min_samples_split": [15, 20],
             "n_estimators": [200, 300]}

xgboost_params = {"learning_rate": [0.1, 0.01],
                  "max_depth": [5, 8],
                  "n_estimators": [100, 200]}

lightgbm_params = {"learning_rate": [0.01, 0.1],
                   "n_estimators": [300, 500]}


classifiers = [("RF", RandomForestClassifier(), rf_params),
               ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgboost_params),
               ('LightGBM', LGBMClassifier(), lightgbm_params)]

def hyperparameter_optimization(X, y, cv=3, scoring="roc_auc"):
    print("Hyperparameter Optimization....")
    best_models = {}
    for name, classifier, params in classifiers:
        print(f"########## {name} ##########")
        cv_results = cross_validate(classifier, X, y, cv=cv, scoring=scoring)
        print(f"{scoring} (Before): {round(cv_results['test_score'].mean(), 4)}")

        gs_best = GridSearchCV(classifier, params, cv=cv, n_jobs=-1, verbose=False).fit(X, y)
        final_model = classifier.set_params(**gs_best.best_params_)

        cv_results = cross_validate(final_model, X, y, cv=cv, scoring=scoring)
        print(f"{scoring} (After): {round(cv_results['test_score'].mean(), 4)}")
        print(f"{name} best params: {gs_best.best_params_}", end="\n\n")
        best_models[name] = final_model
    return best_models

best_models = hyperparameter_optimization(X, y,scoring="recall")

def voting_classifier(best_models, X, y):
    print("Voting Classifier...")
    voting_clf = VotingClassifier(estimators=[('XGBoost', best_models["XGBoost"]), ('RF', best_models["RF"]),
                                              ('LightGBM', best_models["LightGBM"])],
                                  voting='soft').fit(X, y)
    cv_results = cross_validate(voting_clf, X, y, cv=3, scoring=["accuracy", "f1", "roc_auc"])
    print(f"Accuracy: {cv_results['test_accuracy'].mean()}")
    print(f"F1Score: {cv_results['test_f1'].mean()}")
    print(f"ROC_AUC: {cv_results['test_roc_auc'].mean()}")
    return voting_clf